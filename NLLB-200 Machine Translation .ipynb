{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Giriş yap (gerekiyorsa)\n",
        "# login(token=\"HF_TOKENINIZ\")\n",
        "\n",
        "# Yeni model seçimi\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # 600 milyon parametreli versiyon\n",
        "# model_name = \"facebook/nllb-200-1.3B\"  # Daha büyük versiyon (1.3 milyar parametre)\n",
        "\n",
        "# Model ve tokenizer yükleme\n",
        "print(f\"{model_name} modeli yükleniyor...\")\n",
        "start_load = time.time()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(f\"Model yükleme süresi: {time.time()-start_load:.2f}s\")\n",
        "print(f\"Model boyutu: {model.num_parameters()/1e6:.1f}M parametre\")\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "# Çeviri fonksiyonu (İngilizce → Türkçe)\n",
        "def translate_to_turkish(text):\n",
        "    # NLLB'de Türkçe için dil kodu: 'tur_Latn'\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    # NLLB özel parametreleri\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"tur_Latn\")  # Düzeltme burada\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# Test cümlesi\n",
        "test_text = \"This is a sample sentence to test the translation model.\"\n",
        "print(\"\\nTest çevirisi:\")\n",
        "print(f\"Orijinal: {test_text}\")\n",
        "print(f\"Çeviri: {translate_to_turkish(test_text)}\")\n",
        "\n",
        "# Veri setini yükleme ve çeviri\n",
        "df = pd.read_csv('test_dataset_500_sentences.csv')  # Aynı veri seti\n",
        "total_sentences = len(df)\n",
        "print(f\"\\nToplam {total_sentences} cümle çevirisi başlatılıyor...\")\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i, sentence in enumerate(df['Sentence'], 1):\n",
        "    start = time.time()\n",
        "    translation = translate_to_turkish(sentence)\n",
        "    elapsed = time.time() - start\n",
        "    results.append({'orijinal': sentence, 'çeviri': translation, 'süre': elapsed})\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"{i}/{total_sentences} cümle çevrildi | Son cümle süresi: {elapsed:.3f}s\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nToplam süre: {total_time:.2f}s\")\n",
        "print(f\"Ortalama süre/cümle: {total_time/total_sentences:.3f}s\")\n",
        "\n",
        "# Sonuçları kaydet\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(f'nllb_results_{model_name.split(\"/\")[-1]}.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"Çeviriler kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oilt4T5n6kRr",
        "outputId": "b8b957a0-889c-46f6-9646-736bc16717c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "facebook/nllb-200-distilled-600M modeli yükleniyor...\n",
            "Model yükleme süresi: 6.51s\n",
            "Model boyutu: 615.1M parametre\n",
            "Kullanılan cihaz: cuda\n",
            "\n",
            "Test çevirisi:\n",
            "Orijinal: This is a sample sentence to test the translation model.\n",
            "Çeviri: Bu, çeviri modelini test etmek için örnek bir cümle.\n",
            "\n",
            "Toplam 500 cümle çevirisi başlatılıyor...\n",
            "50/500 cümle çevrildi | Son cümle süresi: 0.473s\n",
            "100/500 cümle çevrildi | Son cümle süresi: 0.250s\n",
            "150/500 cümle çevrildi | Son cümle süresi: 0.219s\n",
            "200/500 cümle çevrildi | Son cümle süresi: 0.443s\n",
            "250/500 cümle çevrildi | Son cümle süresi: 0.247s\n",
            "300/500 cümle çevrildi | Son cümle süresi: 0.296s\n",
            "350/500 cümle çevrildi | Son cümle süresi: 0.249s\n",
            "400/500 cümle çevrildi | Son cümle süresi: 0.365s\n",
            "450/500 cümle çevrildi | Son cümle süresi: 0.229s\n",
            "500/500 cümle çevrildi | Son cümle süresi: 0.271s\n",
            "\n",
            "Toplam süre: 149.93s\n",
            "Ortalama süre/cümle: 0.300s\n",
            "Çeviriler kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, ModelInfo\n",
        "import os\n",
        "\n",
        "# Model bilgisi\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "api = HfApi()\n",
        "\n",
        "# 1. Model repo bilgilerini al\n",
        "try:\n",
        "    model_info = api.model_info(model_name)\n",
        "    files = model_info.siblings  # Repodaki tüm dosyalar\n",
        "\n",
        "    print(f\"\\n=== {model_name} Model Dosyaları ===\")\n",
        "    total_size = 0\n",
        "\n",
        "    for file in files:\n",
        "        size_mb = file.size / (1024 * 1024)\n",
        "        print(f\"{file.rfilename.ljust(40)}: {size_mb:>7.2f} MB\")\n",
        "        total_size += file.size\n",
        "\n",
        "    print(f\"\\nToplam Boyut: {total_size/(1024*1024):.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Hata oluştu: {str(e)}\")\n",
        "\n",
        "# 2. Lokal cache boyutunu kontrol et\n",
        "from transformers import TRANSFORMERS_CACHE\n",
        "cache_dir = os.path.join(TRANSFORMERS_CACHE, f\"models--{model_name.replace('/', '--')}\")\n",
        "\n",
        "if os.path.exists(cache_dir):\n",
        "    print(\"\\n=== Lokal Cache Boyutu ===\")\n",
        "    total_local = 0\n",
        "    for root, _, files in os.walk(cache_dir):\n",
        "        for file in files:\n",
        "            fp = os.path.join(root, file)\n",
        "            total_local += os.path.getsize(fp)\n",
        "    print(f\"{total_local/(1024*1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"\\nLokal cache bulunamadı!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRgIKiTW6pQe",
        "outputId": "03b5c3f6-1308-4000-e0ce-152f66ce62a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== facebook/nllb-200-distilled-600M Model Dosyaları ===\n",
            "Hata oluştu: unsupported operand type(s) for /: 'NoneType' and 'int'\n",
            "\n",
            "=== Lokal Cache Boyutu ===\n",
            "9428.03 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml\n",
        "import pynvml\n",
        "\n",
        "def monitor_gpu():\n",
        "    pynvml.nvmlInit()\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "    print(\"\\n=== GPU Bilgileri ===\")\n",
        "    # Model yüklenmeden önce\n",
        "    pre_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used/1024**2\n",
        "\n",
        "    # Modeli yükle\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Model yüklendikten sonra\n",
        "    post_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used/1024**2\n",
        "    print(f\"GPU Bellek Kullanımı:\")\n",
        "    print(f\"  Model yükleme öncesi: {pre_mem:.2f} MB\")\n",
        "    print(f\"  Model yükleme sonrası: {post_mem:.2f} MB\")\n",
        "    print(f\"  Modelin VRAM kullanımı: {post_mem-pre_mem:.2f} MB\")\n",
        "\n",
        "    # Çeviri sırasında maksimum kullanım\n",
        "    max_mem = pre_mem\n",
        "    test_text = \"This is a long sentence to test memory usage during translation. \" * 10\n",
        "    for _ in range(10):\n",
        "        _ = translate_to_turkish(test_text)\n",
        "        current_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used/1024**2\n",
        "        max_mem = max(max_mem, current_mem)\n",
        "\n",
        "    print(f\"  Maksimum gözlemlenen kullanım: {max_mem:.2f} MB\")\n",
        "    pynvml.nvmlShutdown()\n",
        "\n",
        "monitor_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUYqs_EJ6wYe",
        "outputId": "28ff2c2a-b3e4-4b05-c75b-4cc42eb4e7df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.570.86)\n",
            "\n",
            "=== GPU Bilgileri ===\n",
            "GPU Bellek Kullanımı:\n",
            "  Model yükleme öncesi: 5153.88 MB\n",
            "  Model yükleme sonrası: 5153.88 MB\n",
            "  Modelin VRAM kullanımı: 0.00 MB\n",
            "  Maksimum gözlemlenen kullanım: 5205.88 MB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}